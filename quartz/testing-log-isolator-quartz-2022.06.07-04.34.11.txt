# ----- Automated testing logfile (2022-06-07 04:34) ----
# Automated testing logging for testing script (test-quartz.sh)
Output from Quartz batch job for test (serial-3d-combustion):
Running with EMIRGE_HOME=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge
Activating 'quartz.testing.env' environment for '/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/conda'.
/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/bin/python
# conda environments:
#
base                     /p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3
quartz.testing.env    *  /p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/abate.mtc_euler.env
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/ceesd.master
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/chem.master
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/dgfem
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/mc.master

SLURM_NODELIST=quartz1
LMOD_FAMILY_COMPILER_VERSION=8.1.0
PROJ_LIB=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/share/proj
SLURM_JOB_NAME=serial-3d-combustion-quartz-batch.sh
MANPATH=/usr/tce/packages/cmake/cmake-3.18.0/share/man:/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/share/man:/usr/tce/packages/gcc/gcc-8.1.0/man:/usr/tce/man:/usr/share/lmod/lmod/share/man:/usr/tce/packages/dotkit/dotkit/man:/usr/man:/usr/share/man:/usr/local/man:/usr/X11R6/man:/usr/lib64/mvapich/default/man
SLURMD_NODENAME=quartz1
SLURM_TOPOLOGY_ADDR=quartz1
_ModuleTable003_=LAp9LApvcGVubXBpID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db21waWxlci9nY2MvOC4xLjAvb3Blbm1waS80LjAuMC5sdWEiLApmdWxsTmFtZSA9ICJvcGVubXBpLzQuMC4wIiwKbG9hZE9yZGVyID0gNCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJvcGVubXBpLzQuMC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp0ZXhsaXZlID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db3JlL3RleGxpdmUvMjAxNi5sdWEiLApmdWxsTmFtZSA9ICJ0ZXhsaXZlLzIwMTYiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMSwKc3RhdHVzID0gImFjdGl2ZSIsCnVz
GUESTFISH_INIT=\e[1;34m
HOSTNAME=quartz1
SLURM_PRIO_PROCESS=0
SLURM_NODE_ALIASES=(null)
MPICC=mpicc
__LMOD_REF_COUNT_MODULEPATH=/usr/tce/modulefiles/MPI/openmpi/4.0.0:1;/usr/tce/modulefiles/MPI/gcc/8.1.0/openmpi/4.0.0:1;/usr/tce/modulefiles/Compiler/gcc/8.1.0:1;/collab/usr/global/tools/modulefiles/toss_3_x86_64_ib/Core:1;/usr/tce/modulefiles/Core:1;/usr/apps/modulefiles:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1
TERM=xterm-256color
SHELL=/bin/bash
SLURM_JOB_QOS=Added as default
MY_CEESD_DIR=/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz
LMOD_ROOT=/usr/share/lmod
WISECONFIGDIR=/usr/share/wise2/
HISTSIZE=1000
SLURM_TOPOLOGY_ADDR_PATTERN=node
LMOD_SYSTEM_DEFAULT_MODULES=StdEnv
MODULEPATH_ROOT=/usr/share/modulefiles
TMPDIR=/var/tmp/mtcampbe
SSH_CLIENT=130.126.255.46 38407 22
CONDA_SHLVL=1
CONDA_PROMPT_MODIFIER=
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
LMOD_PKG=/usr/share/lmod/lmod
QTDIR=/usr/lib64/qt-3.3
LMOD_VERSION=8.6.19
QTINC=/usr/lib64/qt-3.3/include
SSH_TTY=/dev/pts/6
LC_ALL=C
SLURM_NNODES=1
USER=mtcampbe
LD_LIBRARY_PATH=/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/lib
LMOD_sys=Linux
LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
_dk_shell=bash
CONDA_EXE=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/conda
PFTP_CONFIG_FILENAME=/etc/pftp_config
ENV=/g/g17/mtcampbe/.bashrc
SLURM_JOBID=9971623
_ModuleTable004_=ZXJOYW1lID0gInRleGxpdmUvMjAxNiIsCndWID0gIjAwMDAwMjAxNi4qemZpbmFsIiwKfSwKfSwKbXBhdGhBID0gewoiL3Vzci90Y2UvbW9kdWxlZmlsZXMvTVBJL29wZW5tcGkvNC4wLjAiLCAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvTVBJL2djYy84LjEuMC9vcGVubXBpLzQuMC4wIgosICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db21waWxlci9nY2MvOC4xLjAiLCAiL2NvbGxhYi91c3IvZ2xvYmFsL3Rvb2xzL21vZHVsZWZpbGVzL3Rvc3NfM194ODZfNjRfaWIvQ29yZSIKLCAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZSIsICIvdXNyL2FwcHMvbW9kdWxlZmlsZXMiLCAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eCIKLCAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9Db3Jl
HOST_GRP=linux
DK_UEQRU=1
_CE_CONDA=
LMOD_FAMILY_MPI_VERSION=4.0.0
LMOD_PREPEND_BLOCK=normal
GUESTFISH_PS1=\[\e[1;32m\]><fs>\[\e[0;31m\] 
SLURM_TASKS_PER_NODE=36
_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJnY2MiLAptcGkgPSAib3Blbm1waSIsCn0sCm1UID0gewpTdGRFbnYgPSB7CmZuID0gIi91c3IvdGNlL21vZHVsZWZpbGVzL0NvcmUvU3RkRW52Lmx1YSIsCmZ1bGxOYW1lID0gIlN0ZEVudiIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0gewpsbW9kID0gewpzdGlja3kgPSAxLAp9LAp9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlN0ZEVudiIsCndWID0gIk0uKnpmaW5hbCIsCn0sCmNtYWtlID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVm
MAIL=/var/spool/mail/mtcampbe
PATH=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/bin:/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin:/usr/tce/packages/cmake/cmake-3.18.0/bin:/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/bin:/usr/tce/packages/gcc/gcc-8.1.0/bin:/usr/tce/bin:/usr/tce/packages/texlive/texlive-2016/2016/bin/x86_64-linux:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
PROJ_NETWORK=ON
GSETTINGS_SCHEMA_DIR=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/share/glib-2.0/schemas
SLURM_WORKING_CLUSTER=quartz:equartz187:6817:9472:101
SLURM_CONF=/etc/slurm/slurm.conf
LCSCHEDCLUSTER=quartz
SLURM_JOB_ID=9971623
CONDA_PREFIX=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env
LMOD_SETTARG_CMD=:
LLNL_LD_STACK=/usr/tce/packages/ld-wrappers/bin/ld_auto_rpath:/usr/tce/packages/ld-wrappers/bin/ld_darshan
SLURM_JOB_USER=mtcampbe
_dk_rl=-1
INPUTRC=/etc/inputrc
PWD=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing
_LMFILES_=/usr/tce/modulefiles/Core/texlive/2016.lua:/usr/tce/modulefiles/Core/StdEnv.lua:/usr/tce/modulefiles/Core/gcc/8.1.0.lua:/usr/tce/modulefiles/Compiler/gcc/8.1.0/openmpi/4.0.0.lua:/usr/tce/modulefiles/Core/cmake/3.18.0.lua
EDITOR=/bin/vi
LANG=en_US.UTF-8
MPIFC=mpif90
MODULEPATH=/usr/tce/modulefiles/MPI/openmpi/4.0.0:/usr/tce/modulefiles/MPI/gcc/8.1.0/openmpi/4.0.0:/usr/tce/modulefiles/Compiler/gcc/8.1.0:/collab/usr/global/tools/modulefiles/toss_3_x86_64_ib/Core:/usr/tce/modulefiles/Core:/usr/apps/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core
SLURM_JOB_UID=7527
_ModuleTable_Sz_=5
LOADEDMODULES=texlive/2016:StdEnv:gcc/8.1.0:openmpi/4.0.0:cmake/3.18.0
KDEDIRS=/usr
GUESTFISH_OUTPUT=\e[0m
DARSHAN_LLNL_LOGDIR=/usr/sonar/ingestion/darshan/quartz/mtcampbe
SLURM_NODEID=0
EMIRGE_HOME=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge
SLURM_SUBMIT_DIR=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing
DK_SUBNODE=bash/toss_3_x86_64_ib:bash:toss_3_x86_64_ib:.
SLURM_TASK_PID=36342
_ModuleTable005_=IiwgIi91c3Ivc2hhcmUvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZTovdXNyL2FwcHMvbW9kdWxlZmlsZXM6L3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIsCn0K
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
SLURM_CPUS_ON_NODE=36
_CE_M=
SLURM_PROCID=0
CXX=mpicxx
SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass
_dk_inuse=lcinit.0 dkcomplete.1 reuse.1
ENVIRONMENT=BATCH
HISTCONTROL=ignoredups
KRB5CCNAME=FILE:/tmp/krb5cc_7527_bauo8a
SLURM_JOB_NODELIST=quartz1
SHLVL=5
HOME=/g/g17/mtcampbe
SLURM_LOCALID=0
__LMOD_REF_COUNT_PATH=/usr/tce/packages/cmake/cmake-3.18.0/bin:1;/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/bin:1;/usr/tce/packages/gcc/gcc-8.1.0/bin:1;/usr/tce/bin:1;/usr/tce/packages/texlive/texlive-2016/2016/bin/x86_64-linux:1;/usr/lib64/qt-3.3/bin:1;/usr/condabin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1
SLURM_JOB_GID=7527
SLURM_JOB_CPUS_PER_NODE=36
SLURM_CLUSTER_NAME=quartz
_ModuleTable002_=aWxlcy9Db3JlL2NtYWtlLzMuMTguMC5sdWEiLApmdWxsTmFtZSA9ICJjbWFrZS8zLjE4LjAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNtYWtlLzMuMTguMCIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMTguKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZS9nY2MvOC4xLjAubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLzguMS4wIiwKbG9hZE9yZGVyID0gMywKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MvOC4xLjAiLAp3ViA9ICIwMDAwMDAwMDguMDAwMDAwMDAxLip6ZmluYWwi
SLURM_GTIDS=0
SLURM_SUBMIT_HOST=quartz2306
FC=mpif90
XDG_CACHE_HOME=/tmp/mtcampbe/xdg-scratch
SLURM_JOB_PARTITION=pdebug
BASH_ENV=/usr/share/lmod/lmod/init/bash
CONDA_PYTHON_EXE=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/python
LMOD_arch=x86_64
LOGNAME=mtcampbe
QTLIB=/usr/lib64/qt-3.3/lib
CVS_RSH=ssh
SLURM_JOB_ACCOUNT=uiuc
XDG_DATA_DIRS=/g/g17/mtcampbe/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
_dk_isatty=1
SSH_CONNECTION=130.126.255.46 38407 134.9.54.15 22
MPIBIND_SPROLOG=
SLURM_JOB_NUM_NODES=1
MODULESHOME=/usr/share/lmod/lmod
SYS_TYPE=toss_3_x86_64_ib
CONDA_DEFAULT_ENV=quartz.testing.env
__LMOD_REF_COUNT_LD_LIBRARY_PATH=/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/lib:1
LMOD_SETTARG_FULL_SUPPORT=no
LESSOPEN=||/usr/bin/lesspipe.sh %s
LMOD_FAMILY_COMPILER=gcc
LMOD_FULL_SETTARG_SUPPORT=no
CC=mpicc
QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
__LMOD_DEFAULT_MODULES_LOADED__=1
__LMOD_REF_COUNT_MANPATH=/usr/tce/packages/cmake/cmake-3.18.0/share/man:1;/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/share/man:1;/usr/tce/packages/gcc/gcc-8.1.0/man:1;/usr/tce/man:1;/usr/share/lmod/lmod/share/man:1;/usr/tce/packages/dotkit/dotkit/man:1;/usr/man:1;/usr/share/man:1;/usr/local/man:1;/usr/X11R6/man:1;/usr/lib64/mvapich/default/man:1
LMOD_DIR=/usr/share/lmod/lmod/libexec
MPIF90=mpif90
GUESTFISH_RESTORE=\e[0m
LMOD_FAMILY_MPI=openmpi
LMOD_COLORIZE=yes
DK_NODE=/usr/global/tools/dotkit
DK_ROOT=/usr/tce/packages/dotkit/dotkit
HISTFILE=/g/g17/mtcampbe/.bash_history
MPICXX=mpicxx
BASH_FUNC_module()=() {  local __lmod_my_status;
 local __lmod_sh_dbg;
 if [ -z "${LMOD_SH_DBG_ON+x}" ]; then
 case "$-" in 
 *v*x*)
 __lmod_sh_dbg='vx'
 ;;
 *v*)
 __lmod_sh_dbg='v'
 ;;
 *x*)
 __lmod_sh_dbg='x'
 ;;
 esac;
 fi;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 set +$__lmod_sh_dbg;
 echo "Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output" 1>&2;
 fi;
 eval "$($LMOD_CMD bash "$@")" && eval $(${LMOD_SETTARG_CMD:-:} -s sh);
 __lmod_my_status=$?;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 echo "Shell debugging restarted" 1>&2;
 set -$__lmod_sh_dbg;
 fi;
 return $__lmod_my_status
}
BASH_FUNC_ml()=() {  eval "$($LMOD_DIR/ml_cmd "$@")"
}
_=/usr/bin/env
*** Running isolator in /p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing/../smoke_test_injection_3d ...
Init phase...\n
Default casename isolator_init
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

2022-06-07 05:42:38,565 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[10151,0],0] (PID 36419)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------

#### Simluation control data: ####
	order = 1
	dimen = 3
#### Simluation control data: ####

#### Simluation setup data: ####
	total_pres_injection = 50400
	total_temp_injection = 300.0
	vel_sigma = 1000
	temp_sigma = 1250
	vel_sigma_injection = 5000
	temp_sigma_injection = 5000
#### Simluation setup data: ####

#### Simluation material properties: ####
	mu = 4.216360056e-05
	kappa = 0.05621788139856423
	Prandtl Number  = 0.75
	nspecies = 7
	full multi-species initialization with pyrometheus eos
#### Simluation material properties: ####
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_init.py", line 1183, in <module>
    main(user_input_file=input_file, actx_class=actx_class, casename=casename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_init.py", line 920, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 9971623.0 ON quartz1 CANCELLED AT 2022-06-07T05:42:45 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: quartz1: task 0: Killed
Run phase...\n
srun: Job 9971623 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 9971623
Default casename isolator
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

2022-06-07 05:42:48,192 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[10151,1],0] (PID 36677)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Shock capturing parameters: alpha 0.1, s0 -4.0, kappa 0.5

#### Simluation control data: ####
	nviz = 100
	nrestart = 100
	nhealth = 1
	nstatus = 1
	current_dt = 1e-10
	t_final = 2e-09
	order = 1
	dimen = 3
	Time integration euler
#### Simluation control data: ####


#### Simluation material properties: ####
	mu = 4.216360056e-05
	kappa = 0.05621788139856423
	Prandtl Number  = 0.75
	nspecies = 7
	species diffusivity = 0.0001
	full multi-species initialization with pyrometheus eos
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_run.py", line 1303, in <module>
    main(restart_filename=restart_filename, target_filename=target_filename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_run.py", line 589, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 9971623.1 ON quartz1 CANCELLED AT 2022-06-07T05:43:27 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: quartz1: task 0: Killed
Tue Jun  7 05:57:37 PDT 2022
Output from Quartz batch job for test (parallel-3d-combustion):
Running with EMIRGE_HOME=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge
Activating 'quartz.testing.env' environment for '/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/conda'.
/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/bin/python
# conda environments:
#
base                     /p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3
quartz.testing.env    *  /p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/abate.mtc_euler.env
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/ceesd.master
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/chem.master
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/dgfem
                         /usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz/Install/conda/envs/mc.master

SLURM_NODELIST=quartz[12-15]
LMOD_FAMILY_COMPILER_VERSION=8.1.0
PROJ_LIB=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/share/proj
SLURM_JOB_NAME=parallel-3d-combustion-quartz-batch.sh
MANPATH=/usr/tce/packages/cmake/cmake-3.18.0/share/man:/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/share/man:/usr/tce/packages/gcc/gcc-8.1.0/man:/usr/tce/man:/usr/share/lmod/lmod/share/man:/usr/tce/packages/dotkit/dotkit/man:/usr/man:/usr/share/man:/usr/local/man:/usr/X11R6/man:/usr/lib64/mvapich/default/man
SLURMD_NODENAME=quartz12
SLURM_TOPOLOGY_ADDR=quartz12
_ModuleTable003_=LAp9LApvcGVubXBpID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db21waWxlci9nY2MvOC4xLjAvb3Blbm1waS80LjAuMC5sdWEiLApmdWxsTmFtZSA9ICJvcGVubXBpLzQuMC4wIiwKbG9hZE9yZGVyID0gNCwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJvcGVubXBpLzQuMC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp0ZXhsaXZlID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db3JlL3RleGxpdmUvMjAxNi5sdWEiLApmdWxsTmFtZSA9ICJ0ZXhsaXZlLzIwMTYiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMSwKc3RhdHVzID0gImFjdGl2ZSIsCnVz
GUESTFISH_INIT=\e[1;34m
HOSTNAME=quartz12
SLURM_PRIO_PROCESS=0
SLURM_NODE_ALIASES=(null)
MPICC=mpicc
__LMOD_REF_COUNT_MODULEPATH=/usr/tce/modulefiles/MPI/openmpi/4.0.0:1;/usr/tce/modulefiles/MPI/gcc/8.1.0/openmpi/4.0.0:1;/usr/tce/modulefiles/Compiler/gcc/8.1.0:1;/collab/usr/global/tools/modulefiles/toss_3_x86_64_ib/Core:1;/usr/tce/modulefiles/Core:1;/usr/apps/modulefiles:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1
TERM=xterm-256color
SHELL=/bin/bash
SLURM_JOB_QOS=Added as default
MY_CEESD_DIR=/usr/workspace/wsa/xpacc/Users/mtcampbe/CEESD/Quartz
LMOD_ROOT=/usr/share/lmod
WISECONFIGDIR=/usr/share/wise2/
HISTSIZE=1000
SLURM_TOPOLOGY_ADDR_PATTERN=node
LMOD_SYSTEM_DEFAULT_MODULES=StdEnv
MODULEPATH_ROOT=/usr/share/modulefiles
TMPDIR=/var/tmp/mtcampbe
SSH_CLIENT=130.126.255.46 38407 22
CONDA_SHLVL=1
CONDA_PROMPT_MODIFIER=
GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=
LMOD_PKG=/usr/share/lmod/lmod
QTDIR=/usr/lib64/qt-3.3
LMOD_VERSION=8.6.19
QTINC=/usr/lib64/qt-3.3/include
SSH_TTY=/dev/pts/6
LC_ALL=C
SLURM_NNODES=4
USER=mtcampbe
LD_LIBRARY_PATH=/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/lib
LMOD_sys=Linux
LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
_dk_shell=bash
CONDA_EXE=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/conda
PFTP_CONFIG_FILENAME=/etc/pftp_config
ENV=/g/g17/mtcampbe/.bashrc
SLURM_JOBID=9972666
_ModuleTable004_=ZXJOYW1lID0gInRleGxpdmUvMjAxNiIsCndWID0gIjAwMDAwMjAxNi4qemZpbmFsIiwKfSwKfSwKbXBhdGhBID0gewoiL3Vzci90Y2UvbW9kdWxlZmlsZXMvTVBJL29wZW5tcGkvNC4wLjAiLCAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvTVBJL2djYy84LjEuMC9vcGVubXBpLzQuMC4wIgosICIvdXNyL3RjZS9tb2R1bGVmaWxlcy9Db21waWxlci9nY2MvOC4xLjAiLCAiL2NvbGxhYi91c3IvZ2xvYmFsL3Rvb2xzL21vZHVsZWZpbGVzL3Rvc3NfM194ODZfNjRfaWIvQ29yZSIKLCAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZSIsICIvdXNyL2FwcHMvbW9kdWxlZmlsZXMiLCAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eCIKLCAiL3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9Db3Jl
HOST_GRP=linux
DK_UEQRU=1
_CE_CONDA=
LMOD_FAMILY_MPI_VERSION=4.0.0
LMOD_PREPEND_BLOCK=normal
GUESTFISH_PS1=\[\e[1;32m\]><fs>\[\e[0;31m\] 
SLURM_TASKS_PER_NODE=36(x4)
_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJnY2MiLAptcGkgPSAib3Blbm1waSIsCn0sCm1UID0gewpTdGRFbnYgPSB7CmZuID0gIi91c3IvdGNlL21vZHVsZWZpbGVzL0NvcmUvU3RkRW52Lmx1YSIsCmZ1bGxOYW1lID0gIlN0ZEVudiIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0gewpsbW9kID0gewpzdGlja3kgPSAxLAp9LAp9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIlN0ZEVudiIsCndWID0gIk0uKnpmaW5hbCIsCn0sCmNtYWtlID0gewpmbiA9ICIvdXNyL3RjZS9tb2R1bGVm
MAIL=/var/spool/mail/mtcampbe
PATH=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/bin:/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin:/usr/tce/packages/cmake/cmake-3.18.0/bin:/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/bin:/usr/tce/packages/gcc/gcc-8.1.0/bin:/usr/tce/bin:/usr/tce/packages/texlive/texlive-2016/2016/bin/x86_64-linux:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
PROJ_NETWORK=ON
GSETTINGS_SCHEMA_DIR=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/share/glib-2.0/schemas
SLURM_WORKING_CLUSTER=quartz:equartz187:6817:9472:101
SLURM_CONF=/etc/slurm/slurm.conf
LCSCHEDCLUSTER=quartz
SLURM_JOB_ID=9972666
CONDA_PREFIX=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env
LMOD_SETTARG_CMD=:
LLNL_LD_STACK=/usr/tce/packages/ld-wrappers/bin/ld_auto_rpath:/usr/tce/packages/ld-wrappers/bin/ld_darshan
SLURM_JOB_USER=mtcampbe
_dk_rl=-1
INPUTRC=/etc/inputrc
PWD=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing
_LMFILES_=/usr/tce/modulefiles/Core/texlive/2016.lua:/usr/tce/modulefiles/Core/StdEnv.lua:/usr/tce/modulefiles/Core/gcc/8.1.0.lua:/usr/tce/modulefiles/Compiler/gcc/8.1.0/openmpi/4.0.0.lua:/usr/tce/modulefiles/Core/cmake/3.18.0.lua
EDITOR=/bin/vi
LANG=en_US.UTF-8
MPIFC=mpif90
MODULEPATH=/usr/tce/modulefiles/MPI/openmpi/4.0.0:/usr/tce/modulefiles/MPI/gcc/8.1.0/openmpi/4.0.0:/usr/tce/modulefiles/Compiler/gcc/8.1.0:/collab/usr/global/tools/modulefiles/toss_3_x86_64_ib/Core:/usr/tce/modulefiles/Core:/usr/apps/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core
SLURM_JOB_UID=7527
_ModuleTable_Sz_=5
LOADEDMODULES=texlive/2016:StdEnv:gcc/8.1.0:openmpi/4.0.0:cmake/3.18.0
KDEDIRS=/usr
GUESTFISH_OUTPUT=\e[0m
DARSHAN_LLNL_LOGDIR=/usr/sonar/ingestion/darshan/quartz/mtcampbe
SLURM_NODEID=0
EMIRGE_HOME=/p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge
SLURM_SUBMIT_DIR=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing
DK_SUBNODE=bash/toss_3_x86_64_ib:bash:toss_3_x86_64_ib:.
SLURM_TASK_PID=15124
_ModuleTable005_=IiwgIi91c3Ivc2hhcmUvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZTovdXNyL2FwcHMvbW9kdWxlZmlsZXM6L3Vzci9zaGFyZS9tb2R1bGVmaWxlcy9MaW51eDovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0NvcmU6L3Vzci9zaGFyZS9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIsCn0K
LMOD_CMD=/usr/share/lmod/lmod/libexec/lmod
SLURM_CPUS_ON_NODE=36
_CE_M=
SLURM_PROCID=0
CXX=mpicxx
SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass
_dk_inuse=lcinit.0 dkcomplete.1 reuse.1
ENVIRONMENT=BATCH
HISTCONTROL=ignoredups
KRB5CCNAME=FILE:/tmp/krb5cc_7527_bauo8a
SLURM_JOB_NODELIST=quartz[12-15]
SHLVL=5
HOME=/g/g17/mtcampbe
SLURM_LOCALID=0
__LMOD_REF_COUNT_PATH=/usr/tce/packages/cmake/cmake-3.18.0/bin:1;/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/bin:1;/usr/tce/packages/gcc/gcc-8.1.0/bin:1;/usr/tce/bin:1;/usr/tce/packages/texlive/texlive-2016/2016/bin/x86_64-linux:1;/usr/lib64/qt-3.3/bin:1;/usr/condabin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1
SLURM_JOB_GID=7527
SLURM_JOB_CPUS_PER_NODE=36(x4)
SLURM_CLUSTER_NAME=quartz
_ModuleTable002_=aWxlcy9Db3JlL2NtYWtlLzMuMTguMC5sdWEiLApmdWxsTmFtZSA9ICJjbWFrZS8zLjE4LjAiLApsb2FkT3JkZXIgPSA1LApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNtYWtlLzMuMTguMCIsCndWID0gIjAwMDAwMDAwMy4wMDAwMDAwMTguKnpmaW5hbCIsCn0sCmdjYyA9IHsKZm4gPSAiL3Vzci90Y2UvbW9kdWxlZmlsZXMvQ29yZS9nY2MvOC4xLjAubHVhIiwKZnVsbE5hbWUgPSAiZ2NjLzguMS4wIiwKbG9hZE9yZGVyID0gMywKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJnY2MvOC4xLjAiLAp3ViA9ICIwMDAwMDAwMDguMDAwMDAwMDAxLip6ZmluYWwi
SLURM_GTIDS=0
SLURM_SUBMIT_HOST=quartz2306
FC=mpif90
XDG_CACHE_HOME=/tmp/mtcampbe/xdg-scratch
SLURM_JOB_PARTITION=pdebug
BASH_ENV=/usr/share/lmod/lmod/init/bash
CONDA_PYTHON_EXE=/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/bin/python
LMOD_arch=x86_64
LOGNAME=mtcampbe
QTLIB=/usr/lib64/qt-3.3/lib
CVS_RSH=ssh
SLURM_JOB_ACCOUNT=uiuc
XDG_DATA_DIRS=/g/g17/mtcampbe/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
_dk_isatty=1
SSH_CONNECTION=130.126.255.46 38407 134.9.54.15 22
MPIBIND_SPROLOG=
SLURM_JOB_NUM_NODES=4
MODULESHOME=/usr/share/lmod/lmod
SYS_TYPE=toss_3_x86_64_ib
CONDA_DEFAULT_ENV=quartz.testing.env
__LMOD_REF_COUNT_LD_LIBRARY_PATH=/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/lib:1
LMOD_SETTARG_FULL_SUPPORT=no
LESSOPEN=||/usr/bin/lesspipe.sh %s
LMOD_FAMILY_COMPILER=gcc
LMOD_FULL_SETTARG_SUPPORT=no
CC=mpicc
QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
__LMOD_DEFAULT_MODULES_LOADED__=1
__LMOD_REF_COUNT_MANPATH=/usr/tce/packages/cmake/cmake-3.18.0/share/man:1;/usr/tce/packages/openmpi/openmpi-4.0.0-gcc-8.1.0/share/man:1;/usr/tce/packages/gcc/gcc-8.1.0/man:1;/usr/tce/man:1;/usr/share/lmod/lmod/share/man:1;/usr/tce/packages/dotkit/dotkit/man:1;/usr/man:1;/usr/share/man:1;/usr/local/man:1;/usr/X11R6/man:1;/usr/lib64/mvapich/default/man:1
LMOD_DIR=/usr/share/lmod/lmod/libexec
MPIF90=mpif90
GUESTFISH_RESTORE=\e[0m
LMOD_FAMILY_MPI=openmpi
LMOD_COLORIZE=yes
DK_NODE=/usr/global/tools/dotkit
DK_ROOT=/usr/tce/packages/dotkit/dotkit
HISTFILE=/g/g17/mtcampbe/.bash_history
MPICXX=mpicxx
BASH_FUNC_module()=() {  local __lmod_my_status;
 local __lmod_sh_dbg;
 if [ -z "${LMOD_SH_DBG_ON+x}" ]; then
 case "$-" in 
 *v*x*)
 __lmod_sh_dbg='vx'
 ;;
 *v*)
 __lmod_sh_dbg='v'
 ;;
 *x*)
 __lmod_sh_dbg='x'
 ;;
 esac;
 fi;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 set +$__lmod_sh_dbg;
 echo "Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output" 1>&2;
 fi;
 eval "$($LMOD_CMD bash "$@")" && eval $(${LMOD_SETTARG_CMD:-:} -s sh);
 __lmod_my_status=$?;
 if [ -n "${__lmod_sh_dbg:-}" ]; then
 echo "Shell debugging restarted" 1>&2;
 set -$__lmod_sh_dbg;
 fi;
 return $__lmod_my_status
}
BASH_FUNC_ml()=() {  eval "$($LMOD_DIR/ml_cmd "$@")"
}
_=/usr/bin/env
*** Running 2 rank isolator in /p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing/../smoke_test_injection_3d ...
Init phase...\n
srun: Warning: can't run 2 processes on 4 nodes, setting nnodes to 2
Default casename isolator_init
Default casename isolator_init
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

2022-06-07 05:58:33,079 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
2022-06-07 05:58:33,080 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,0],0] (PID 15218)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,0],1] (PID 17575)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------

#### Simluation control data: ####
	order = 1
	dimen = 3
#### Simluation control data: ####

#### Simluation setup data: ####
	total_pres_injection = 50400
	total_temp_injection = 300.0
	vel_sigma = 1000
	temp_sigma = 1250
	vel_sigma_injection = 5000
	temp_sigma_injection = 5000
#### Simluation setup data: ####

#### Simluation material properties: ####
	mu = 4.216360056e-05
	kappa = 0.05621788139856423
	Prandtl Number  = 0.75
	nspecies = 7
	full multi-species initialization with pyrometheus eos
#### Simluation material properties: ####
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_init.py", line 1183, in <module>
    exec(code, run_globals)
  File "./isolator_injection_init.py", line 1183, in <module>
    main(user_input_file=input_file, actx_class=actx_class, casename=casename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    main(user_input_file=input_file, actx_class=actx_class, casename=casename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_init.py", line 920, in main
    func(*args, **kwargs)
  File "./isolator_injection_init.py", line 920, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 9972666.0 ON quartz12 CANCELLED AT 2022-06-07T05:58:39 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: quartz12: task 0: Killed
srun: error: quartz13: task 1: Killed
Run phase...\n
srun: Warning: can't run 2 processes on 4 nodes, setting nnodes to 2
Default casename isolator
Default casename isolator
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

2022-06-07 05:58:46,983 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

2022-06-07 05:58:46,988 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,1],1] (PID 16314)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,1],0] (PID 67857)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Shock capturing parameters: alpha 0.1, s0 -4.0, kappa 0.5

#### Simluation control data: ####
	nviz = 100
	nrestart = 100
	nhealth = 1
	nstatus = 1
	current_dt = 1e-10
	t_final = 2e-09
	order = 1
	dimen = 3
	Time integration euler
#### Simluation control data: ####


#### Simluation material properties: ####
	mu = 4.216360056e-05
	kappa = 0.05621788139856423
	Prandtl Number  = 0.75
	nspecies = 7
	species diffusivity = 0.0001
	full multi-species initialization with pyrometheus eos
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    exec(code, run_globals)
  File "./isolator_injection_run.py", line 1303, in <module>
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_run.py", line 1303, in <module>
    main(restart_filename=restart_filename, target_filename=target_filename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    main(restart_filename=restart_filename, target_filename=target_filename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_run.py", line 589, in main
    func(*args, **kwargs)
  File "./isolator_injection_run.py", line 589, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 9972666.1 ON quartz14 CANCELLED AT 2022-06-07T06:00:09 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: quartz14: task 0: Killed
srun: error: quartz15: task 1: Killed
*** Running 4 rank isolator in /p/lscratchh/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/isolator/testing/../smoke_test_injection_3d ...
Init phase...\n
Default casename isolator_init
Default casename isolator_init
2022-06-07 06:00:11,553 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

2022-06-07 06:00:11,558 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Default casename isolator_init
Default casename isolator_init
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

2022-06-07 06:00:11,605 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_init.py

2022-06-07 06:00:11,620 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,2],3] (PID 17050)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,2],1] (PID 17935)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,2],0] (PID 15611)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_init.py", line 1183, in <module>
    exec(code, run_globals)
  File "./isolator_injection_init.py", line 1183, in <module>
    main(user_input_file=input_file, actx_class=actx_class, casename=casename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    main(user_input_file=input_file, actx_class=actx_class, casename=casename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_init.py", line 920, in main
    func(*args, **kwargs)
  File "./isolator_injection_init.py", line 920, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,2],2] (PID 68601)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 9972666.2 ON quartz12 CANCELLED AT 2022-06-07T06:00:12 ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
srun: error: quartz12: task 0: Killed
srun: error: quartz13: task 1: Killed
srun: error: quartz15: task 3: Killed
srun: error: quartz14: task 2: Killed
Run phase...\n
Default casename isolator
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

2022-06-07 06:00:14,230 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Default casename isolator
Default casename isolator
Restarting from file: restart_data/isolator_init-000000
2022-06-07 06:00:14,311 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

Default casename isolator
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py

2022-06-07 06:00:14,323 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True
Restarting from file: restart_data/isolator_init-000000
Ignoring user input from file: run_params.yaml
Running ./isolator_injection_run.py
2022-06-07 06:00:14,359 - INFO - grudge.array_context - get_reasonable_array_context_class: MPIFusionContractorArrayContext lazy=True distributed=True device-parallel=True

--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,3],3] (PID 17253)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,3],1] (PID 18138)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,3],0] (PID 15834)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[11194,3],2] (PID 68726)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/__main__.py", line 7, in <module>
    main()
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 198, in main
    run_command_line(args)
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/site-packages/mpi4py/run.py", line 47, in run_command_line
    run_path(sys.argv[0], run_name='__main__')
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 288, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/miniforge3/envs/quartz.testing.env/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "./isolator_injection_run.py", line 1303, in <module>
    main(restart_filename=restart_filename, target_filename=target_filename,
  File "/p/lustre2/mtcampbe/CEESD/AutomatedTesting/testing/emirge/mirgecom/mirgecom/mpi.py", line 157, in wrapped_func
    func(*args, **kwargs)
  File "./isolator_injection_run.py", line 589, in main
    from mirgecom.mechanisms.uiuc import Thermochemistry
ModuleNotFoundError: No module named 'mirgecom.mechanisms.uiuc'
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** STEP 9972666.3 ON quartz12 CANCELLED AT 2022-06-07T06:00:47 ***
srun: error: quartz12: task 0: Killed
srun: error: quartz15: task 3: Killed
srun: error: quartz14: task 2: Killed
srun: error: quartz13: task 1: Killed
Tue Jun  7 06:28:08 PDT 2022
# Return status for testing script (test-quartz.sh): 0
